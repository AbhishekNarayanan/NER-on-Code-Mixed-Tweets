{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP PROJECT FOR UE16CS333 BY TEAM 13\n",
    "## Team Members :\n",
    "## Abhishek Narayanan (01FB16ECS016)\n",
    "## Abhishek Prasad (01FB16ECS017)\n",
    "## Abijna Rao (01FB16ECS019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character level BI-Directional LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\Abhishek\\\\Desktop\\\\Named-Entity-Recognition\\\\Twitterdata\\\\annotatedData.csv\")\n",
    "data = data.fillna(method=\"ffill\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sent</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72133</th>\n",
       "      <td>sent: 3637</td>\n",
       "      <td>bn</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72134</th>\n",
       "      <td>sent: 3637</td>\n",
       "      <td>koi</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72135</th>\n",
       "      <td>sent: 3637</td>\n",
       "      <td>dokhe</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72136</th>\n",
       "      <td>sent: 3637</td>\n",
       "      <td>se</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72137</th>\n",
       "      <td>sent: 3637</td>\n",
       "      <td>idhr</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72138</th>\n",
       "      <td>sent: 3637</td>\n",
       "      <td>udhr</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72139</th>\n",
       "      <td>sent: 3637</td>\n",
       "      <td>kar</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72140</th>\n",
       "      <td>sent: 3637</td>\n",
       "      <td>gya</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72141</th>\n",
       "      <td>sent: 3637</td>\n",
       "      <td>#pnb</td>\n",
       "      <td>B-Org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72142</th>\n",
       "      <td>sent: 3637</td>\n",
       "      <td>#fraud</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Sent    Word    Tag\n",
       "72133  sent: 3637      bn  Other\n",
       "72134  sent: 3637     koi  Other\n",
       "72135  sent: 3637   dokhe  Other\n",
       "72136  sent: 3637      se  Other\n",
       "72137  sent: 3637    idhr  Other\n",
       "72138  sent: 3637    udhr  Other\n",
       "72139  sent: 3637     kar  Other\n",
       "72140  sent: 3637     gya  Other\n",
       "72141  sent: 3637    #pnb  B-Org\n",
       "72142  sent: 3637  #fraud  Other"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14866"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(data[\"Word\"].values))\n",
    "n_words = len(words); n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags = len(tags); n_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form Sentences using sentence numbers in the word tag list provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sent\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"sent: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = getter.get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('agar', 'Other'), ('#notebandi', 'Other'), ('ke', 'Other'), ('time', 'Other'), ('political', 'B-Org'), ('party', 'I-Org'), ('bhi', 'Other'), ('#rti', 'Other'), ('ke', 'Other'), ('daayre', 'Other'), ('me', 'Other'), ('aa', 'Other'), ('jati', 'Other'), ('to', 'Other'), ('#sukmaattack', 'Other'), ('#kashmir', 'B-Loc'), ('me', 'Other'), ('patthar', 'Other'), ('attack', 'Other'), ('na', 'Other'), ('hote', 'Other'), ('@PMOIndia', 'Other'), ('@PMOIndia', 'Other')]\n"
     ]
    }
   ],
   "source": [
    "print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 75\n",
    "max_len_char = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map word to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"UNK\"] = 1\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1925\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(word2idx[\"agar\"])\n",
    "print(tag2idx[\"B-Org\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_word = [[word2idx[w[0]] for w in s] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_word = pad_sequences(maxlen=max_len, sequences=X_word, value=word2idx[\"PAD\"], padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad word sequences to form uniform length vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "chars = set([w_i for w in words for w_i in w])\n",
    "n_chars = len(chars)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char2idx = {c: i + 2 for i, c in enumerate(chars)}\n",
    "char2idx[\"UNK\"] = 1\n",
    "char2idx[\"PAD\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_char = []\n",
    "for sentence in sentences:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"PAD\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    X_char.append(np.array(sent_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map tag labels to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = pad_sequences(maxlen=max_len, sequences=y, value=tag2idx[\"PAD\"], padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_word_tr, X_word_te, y_tr, y_te = train_test_split(X_word, y, test_size=0.1, random_state=2018)\n",
    "X_char_tr, X_char_te, _, _ = train_test_split(X_char, y, test_size=0.1, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level Bidirectional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input and embedding for words\n",
    "word_in = Input(shape=(max_len,))\n",
    "emb_word = Embedding(input_dim=n_words + 2, output_dim=20,\n",
    "                     input_length=max_len, mask_zero=True)(word_in)\n",
    "\n",
    "# input and embeddings for characters\n",
    "char_in = Input(shape=(max_len, max_len_char,))\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars + 2, output_dim=10,\n",
    "                           input_length=max_len_char, mask_zero=True))(char_in)\n",
    "# character LSTM to get word encodings by characters\n",
    "char_enc = TimeDistributed(LSTM(units=20, return_sequences=False,\n",
    "                                recurrent_dropout=0.5))(emb_char)\n",
    "\n",
    "# main LSTM\n",
    "x = concatenate([emb_word, char_enc])\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "main_lstm = Bidirectional(LSTM(units=50, return_sequences=True,\n",
    "                               recurrent_dropout=0.6))(x)\n",
    "out = TimeDistributed(Dense(n_tags + 1, activation=\"softmax\"))(main_lstm)\n",
    "\n",
    "model = Model([word_in, char_in], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 75, 10)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 75, 10, 10)   1100        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 75, 20)       297360      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 75, 20)       2480        time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 75, 40)       0           embedding_1[0][0]                \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 75, 40)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 75, 100)      36400       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 75, 8)        808         bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 338,148\n",
      "Trainable params: 338,148\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2498 samples, validate on 278 samples\n",
      "Epoch 1/20\n",
      "2498/2498 [==============================] - 20s 8ms/step - loss: 0.7977 - acc: 0.9139 - val_loss: 0.3923 - val_acc: 0.9203\n",
      "Epoch 2/20\n",
      "2498/2498 [==============================] - 16s 6ms/step - loss: 0.3573 - acc: 0.9266 - val_loss: 0.3590 - val_acc: 0.9203\n",
      "Epoch 3/20\n",
      "2498/2498 [==============================] - 15s 6ms/step - loss: 0.3021 - acc: 0.9267 - val_loss: 0.2710 - val_acc: 0.9210\n",
      "Epoch 4/20\n",
      "2498/2498 [==============================] - 14s 6ms/step - loss: 0.2200 - acc: 0.9373 - val_loss: 0.2254 - val_acc: 0.9354\n",
      "Epoch 5/20\n",
      "2498/2498 [==============================] - 14s 6ms/step - loss: 0.1731 - acc: 0.9468 - val_loss: 0.2059 - val_acc: 0.9382\n",
      "Epoch 6/20\n",
      "2498/2498 [==============================] - 14s 6ms/step - loss: 0.1439 - acc: 0.9536 - val_loss: 0.1963 - val_acc: 0.9407\n",
      "Epoch 7/20\n",
      "2498/2498 [==============================] - 14s 6ms/step - loss: 0.1253 - acc: 0.9595 - val_loss: 0.1958 - val_acc: 0.9430\n",
      "Epoch 8/20\n",
      "2498/2498 [==============================] - 15s 6ms/step - loss: 0.1110 - acc: 0.9639 - val_loss: 0.1894 - val_acc: 0.9462\n",
      "Epoch 9/20\n",
      "2498/2498 [==============================] - 14s 6ms/step - loss: 0.0999 - acc: 0.9669 - val_loss: 0.1884 - val_acc: 0.9494\n",
      "Epoch 10/20\n",
      "2498/2498 [==============================] - 15s 6ms/step - loss: 0.0907 - acc: 0.9704 - val_loss: 0.1872 - val_acc: 0.9518\n",
      "Epoch 11/20\n",
      "2498/2498 [==============================] - 14s 6ms/step - loss: 0.0829 - acc: 0.9737 - val_loss: 0.1864 - val_acc: 0.9531\n",
      "Epoch 12/20\n",
      "2498/2498 [==============================] - 14s 6ms/step - loss: 0.0770 - acc: 0.9758 - val_loss: 0.1848 - val_acc: 0.9532\n",
      "Epoch 13/20\n",
      "2498/2498 [==============================] - 14s 5ms/step - loss: 0.0694 - acc: 0.9788 - val_loss: 0.1948 - val_acc: 0.9518\n",
      "Epoch 14/20\n",
      "2498/2498 [==============================] - 14s 5ms/step - loss: 0.0642 - acc: 0.9801 - val_loss: 0.1944 - val_acc: 0.9540\n",
      "Epoch 15/20\n",
      "2498/2498 [==============================] - 14s 6ms/step - loss: 0.0593 - acc: 0.9822 - val_loss: 0.2003 - val_acc: 0.9546\n",
      "Epoch 16/20\n",
      "2498/2498 [==============================] - 14s 5ms/step - loss: 0.0546 - acc: 0.9835 - val_loss: 0.2004 - val_acc: 0.9555\n",
      "Epoch 17/20\n",
      "2498/2498 [==============================] - 14s 5ms/step - loss: 0.0476 - acc: 0.9855 - val_loss: 0.2083 - val_acc: 0.9547\n",
      "Epoch 18/20\n",
      "2498/2498 [==============================] - 14s 5ms/step - loss: 0.0469 - acc: 0.9861 - val_loss: 0.2120 - val_acc: 0.9554\n",
      "Epoch 19/20\n",
      "2498/2498 [==============================] - 14s 6ms/step - loss: 0.0417 - acc: 0.9874 - val_loss: 0.2193 - val_acc: 0.9565\n",
      "Epoch 20/20\n",
      "2498/2498 [==============================] - 14s 6ms/step - loss: 0.0383 - acc: 0.9883 - val_loss: 0.2266 - val_acc: 0.9563\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_word_tr,\n",
    "                     np.array(X_char_tr).reshape((len(X_char_tr), max_len, max_len_char))],\n",
    "                    np.array(y_tr).reshape(len(y_tr), max_len, 1),\n",
    "                    batch_size=32, epochs=20, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([X_word_te,\n",
    "                        np.array(X_char_te).reshape((len(X_char_te),\n",
    "                                                     max_len, max_len_char))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           ||True ||Pred\n",
      "==============================\n",
      "pagal          : Other Other\n",
      "he             : Other Other\n",
      "vo             : Other Other\n",
      "Israel         : B-Loc B-Loc\n",
      "ne             : Other Other\n",
      "kabi           : Other Other\n",
      "apne           : Other Other\n",
      "country        : Other Other\n",
      "me             : Other Other\n",
      "rahene         : Other Other\n",
      "vale           : Other Other\n",
      "Muslim         : B-Org B-Org\n",
      "ko             : Other Other\n",
      "ya             : Other Other\n",
      "unki           : Other Other\n",
      "masjido        : Other Other\n",
      "ko             : Other Other\n",
      "nai            : Other Other\n",
      "giraya         : Other Other\n",
      "...            : Other Other\n",
      "na             : Other Other\n",
      "hi             : Other Other\n",
      "kisi           : Other Other\n",
      "Muslim         : B-Org B-Org\n",
      "ki             : Other Other\n",
      "ladies         : Other Other\n",
      "pe             : Other Other\n",
      "Rape           : Other Other\n",
      "kiya           : Other Other\n",
      "..             : Other Other\n",
      "vaha           : Other Other\n",
      "Yahudi         : Other Other\n",
      "ladkiya        : Other Other\n",
      "showkh         : Other Other\n",
      "se             : Other Other\n",
      "Muslim         : B-Org B-Org\n",
      "se             : Other Other\n",
      "sadi           : Other Other\n",
      "kar            : Other Other\n",
      "sakti          : Other Other\n",
      "he             : Other Other\n",
      "..             : Other Other\n",
      "but            : Other Other\n",
      "aapne          : Other Other\n",
      "to             : Other Other\n",
      "desh           : B-Org B-Org\n",
      "mehi           : Other Other\n",
      "logo           : Other Other\n",
      "ko             : Other Other\n",
      "marvaya        : Other Other\n",
      "he             : Other Other\n",
      "..             : Other Other\n",
      "..             : Other Other\n"
     ]
    }
   ],
   "source": [
    "i = 250\n",
    "p = np.argmax(y_pred[i], axis=-1)\n",
    "print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(30 * \"=\")\n",
    "for w, t, pred in zip(X_word_te[i], y_te[i], p):\n",
    "    if w != 0:\n",
    "        print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag[t], idx2tag[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      I-Loc       0.79      0.67      0.73       259\n",
      "      B-Org       0.74      0.72      0.73       158\n",
      "      I-Per       0.80      0.65      0.72        69\n",
      "      Other       0.00      0.00      0.00         1\n",
      "      B-Per       0.00      0.00      0.00        12\n",
      "      I-Org       0.53      0.51      0.52        65\n",
      "      B-Loc       0.98      0.99      0.98      6629\n",
      "\n",
      "avg / total       0.96      0.96      0.96      7193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predicted=[ list(np.argmax(y_pred[i],axis=-1)) for i in range(len(y_pred))]\n",
    "correct_flat = [item for sublist in y_te for item in sublist]\n",
    "predicted_flat = [item for sublist in predicted for item in sublist]\n",
    "\n",
    "remove_padded_correct=[correct_flat[i] for i in range(len(correct_flat)) if correct_flat[i] != 0]\n",
    "remove_padded_predicted=[predicted_flat[i] for i in range(len(correct_flat)) if correct_flat[i] != 0 ]\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(remove_padded_correct,remove_padded_predicted, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
